# JAILBREAKS
- L1B3RT4S - https://github.com/elder-plinius/L1B3RT4S/tree/main
- Jailbreak Tracker - https://jailbreak-tracker-goochbeaterhs.replit.app/
- Awesome GPT Super Prompting - https://github.com/CyberAlbSecOP/Awesome_GPT_Super_Prompting
- [Jailbreaking in GenAI: Techniques and Ethical Implications](https://learnprompting.org/docs/prompt_hacking/jailbreaking) – Explores methods and ethics of jailbreaking AI models.

- [Jailbreaking LLMs: A Comprehensive Guide (With Examples)](https://www.promptfoo.dev/blog/how-to-jailbreak-llms/) – Provides examples and strategies for jailbreaking large language models.

- [AI Jailbreak – IBM](https://www.ibm.com/think/insights/ai-jailbreak) – Discusses AI jailbreaks and their implications.

- [AI Jailbreaking Demo: How Prompt Engineering Bypasses LLM Security Measures](https://www.youtube.com/watch?v=F_KychntktU) – Demonstrates techniques to bypass large language model safeguards.

- [Prompt Injection vs. Jailbreaking: What's the Difference?](https://learnprompting.org/blog/injection_jailbreaking) – Clarifies distinctions between prompt injection and jailbreaking in AI.

- [GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253) – Introduces a framework for generating jailbreak prompts to test LLM security.

- [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522) – Discusses a novel approach for manipulating prompts to jailbreak LLMs.

- [SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack](https://arxiv.org/abs/2407.01902) – Presents a framework for designing jailbreak prompts automatically.

- [Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation](https://arxiv.org/abs/2410.11317) – Explores methods to improve jailbreak attacks through adversarial prompt translation.

- [AI Jailbreaks: What They Are and How They Can Be Mitigated](https://www.ibm.com/think/insights/ai-jailbreak) – Discusses the nature of AI jailbreaks and mitigation strategies.
